{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 ANN & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron(퍼셉트론)\n",
    "1958년 Frank Rosenblatt(프랭크 로젠블랫)\n",
    "생물학적 뇌의 뉴런을 모방하여 만든 인공신경망의 기본 단위\n",
    "여러 입력을 값으로 받아 하나의 0 또는 1을 출력하는 함수\n",
    "뇌의 신경세포 즉, 뉴런과 같은 역할\n",
    "![image.png](https://i.imgur.com/Ufrgh5x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "* Single-Layer Perceptron\n",
    "    * 하나의 Perceptron 만을 이용\n",
    "    * Logistic Regression\n",
    "* Multi-Layer Perceptron\n",
    "    * 여러개의 퍼셉트론 이용\n",
    "    * Hidden-Layer\n",
    "        * 정확한 상태를 알 수 없다.\n",
    "\n",
    "![image.png](https://i.imgur.com/ujilihQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network\n",
    "* 인공 신경망\n",
    "* Perceptron을 여러개 연결한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron의 필요성\n",
    "* 비 선형 문제 \n",
    "* 대표적 사례 : XOR 문제\n",
    "* 고차원 다항식\n",
    "![image.png](https://i.imgur.com/YifwZKU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "* Deep Neural Network를 이용한 Machine Learning\n",
    "* Hidden-Layer가 2개 이상인 인공 신경망\n",
    "* 1000개 이상인 경우 도 일반적\n",
    "\n",
    "![image.png](https://i.imgur.com/7TYDnf3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상수 값으로 풀어보는 XOR  예제\n",
    "* 2개 층으로 구성된 NN으로 XOR 문제를 푸는 예시\n",
    "* 단, W와 b를 이미 알고 있는 값을 지정해서 푼다.\n",
    "* 이것이 가능하다면 여러 개층의 W와 b를 GD로 풀수 있으면 많은 문제를 딥러닝으로 해결이 가능하다는 말이다.\n",
    "* 여기서 사용한 W와 b\n",
    "![xor_nn_1](https://user-images.githubusercontent.com/661959/54298177-9e82f080-45fb-11e9-8bdd-1f86718c6f5d.png)\n",
    "* $W$와 $bias$\n",
    "    * Layer-1 y1: $W = \\begin{bmatrix} 5 \\\\5 \\end{bmatrix} , b= -8$\n",
    "    * Layer-1 y2: $W = \\begin{bmatrix}-7 \\\\ -7 \\end{bmatrix} , b= 3$\n",
    "    * Layer-2 : $W= \\begin{bmatrix}-11 \\\\ -11 \\end{bmatrix}, b = 6$\n",
    "* 연산식\n",
    "    * $x=(0,0)$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 = -8, sig(-8)=0$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = 3, sig(3)=1$\n",
    "            * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "    * $x=(0,1)$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,0)$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,1)$\n",
    "        * $ \\begin{bmatrix}1 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5+5-8=2, sig(2)=1$\n",
    "        * $ \\begin{bmatrix}1 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7-7+3=-11, sig(-11)=0$\n",
    "            * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "* 연산 결과\n",
    "\n",
    "| $x_1$ $x_2$ | $y_1$ $y_2$ | $\\hat y$|\n",
    "|---|---|---|\n",
    "|0,0|0,1|0\n",
    "|0,1|0,0|1\n",
    "|1,0|0,0|1\n",
    "|1,1|1,0|0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow를 이용한 XOR 문제 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR 문제 Multi-variable 형식으로 축소\n",
    "![xor_nn_2](https://user-images.githubusercontent.com/661959/54298185-a347a480-45fb-11e9-91d4-e98111241794.png)\n",
    "\n",
    "* $W$와 bias\n",
    "    * $x = \\begin {bmatrix}0 & 0\\\\ 0&1\\\\1&0\\\\1&1 \\end {bmatrix}$\n",
    "    * $W_1 = \\begin{bmatrix}5&-7 \\\\ 5&-7\\end{bmatrix}$\n",
    "    * $b_1 = \\begin{bmatrix}-8 & 3 \\end{bmatrix}$\n",
    "    * $W_2 = \\begin{bmatrix}-11 \\\\ -11\\end{bmatrix}$\n",
    "    * $b_2 = 6 $\n",
    "* 연산식\n",
    "    * $ \\hat y =\n",
    "  \\begin{cases}\n",
    "    K(x) = sigmoid(Xw_1 + b_1)\\\\\n",
    "    H(x) = sigmoid(K(x)W_2 + b_2)\n",
    "  \\end{cases}\n",
    "    $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(np.array([[5,-7], [5,-7]], dtype=np.float32), name='weight1')\n",
    "b1 = tf.Variable(np.array([[-8, 3]], dtype=np.float32), name='bias1')\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(np.array([[-11],[-11]], dtype=np.float32), name='weight2')\n",
    "b2 = tf.Variable(np.array([6], dtype=np.float32), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"\\nHypothesis:\\n{hypothesis} \\nPredicted:\\n{predicted} \\nAccuracy:\\n{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* 역 전파 알고리즘\n",
    "* 출력층의 결과 오차를 입력층 까지 거슬러 전파하면서 계산\n",
    "* 미분 연산\n",
    "    * ReLu 활성화 함수  \n",
    "    * Sigmoid 미분 불가능\n",
    "![image.png](https://i.imgur.com/qdVRwxq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* Chain Rule\n",
    "    * $f(g(x))$, $f(g)$, $g(x)$에 대해 미분\n",
    "        * $\\displaystyle \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x}$\n",
    "![](https://i.imgur.com/uPVRjcD.png)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 문제 MLP 학습 예제\n",
    "* 앞서 상수로 풀었던 XOR 문제를 MLP 학습으로 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([2]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        \n",
    "        d_W1, d_b1, d_W2, d_b2 = tape.gradient(cost, [W1, b1, W2, b2] )\n",
    "        \n",
    "        W1.assign_sub(learning_rate * d_W1)\n",
    "        b1.assign_sub(learning_rate * d_b1)\n",
    "        W2.assign_sub(learning_rate * d_W2)\n",
    "        b2.assign_sub(learning_rate * d_b2)\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f\"step:{step},\\t cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n",
    "print(f\"w1:{W1.numpy()}, b1:{b1.numpy()}, w2:{W2.numpy()}, b2{b2.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient\n",
    "![](https://i.imgur.com/kzQpyNL.png)\n",
    "* sigmoid 함수를 사용하면 input값들이(x1,x2,x3....xn) layer을 거치면서 0에 수렴\n",
    "* 0에 수렴하는 값들이 다른 layer의 input 값으로 입력된다.\n",
    "* 입력된 값들은 layer를 거치면서 0에 수렴\n",
    "* x1,x2,x3....xn의 값은 최종으로 출력 되는 값에 영향이 없다\n",
    "* cost가 줄어들지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(tf.random.normal([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function \n",
    "* Rectified Linear Unit\n",
    "* `max(0,x)` \n",
    "* `tf.nn.relu(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "* 역 전파 알고리즘에 적합한 활성화 함수\n",
    "![image.png](https://i.imgur.com/OzBowJo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(tf.random.normal([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight 초기 값\n",
    "* Geffrey E. Hinton (2006) \" A Fast Learning Algorithm for Deep Belief Nets\"\n",
    "    * http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf\n",
    "    * 0을 사용하지 말것\n",
    "    * RBM(Restricted Boltzmann Machine) 으로 초기화\n",
    "        * 입력 값을 타겟으로 하는 W와 출력을 찾는 사전 훈련\n",
    "        * $  X \\times W = H $\n",
    "        * H와 W를 랜덤하게 초기화 해서 X와의 차이를 계산\n",
    "        * 입력된 X와 차이가 최소가 되는 W로 초기화\n",
    "    * 초창기에 사용\n",
    "* Xavier 초기화\n",
    "    * Xavier Glorot and Yoshua Bengio (2010)\n",
    "        * http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "    * 입력 값 갯수와 출력 값 갯수 사이의 난수를 입력 값 갯수의 제곱근으로 나눈다.\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in)`\n",
    "    * Tensorflow Initializer\n",
    "        * ```initializer= tf.initializers.GlorotUniform()\n",
    "             W = tf.Variable(itntializer(shape=[5,5]))\n",
    "        ```\n",
    "        * ```W = tf.get_variable('W1', shape=[5,5], initializer= tf.glorot_uniform_initializer())```\n",
    "* He 초기화\n",
    "    * Kaiming He (2015)\n",
    "        * https://arxiv.org/abs/1502.01852\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in/2)`\n",
    "    * 입력 값을 2로 나눈 제곱근, 분모가 작아지기 때문에 xavier 보다 넓은 범위의 난수 \n",
    "    * Tensorflow Initializer\n",
    "        * ```tf.initializers.he_normal()```\n",
    "    \n",
    "![](https://i.imgur.com/jrkciOO.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "init = tf.initializers.GlorotUniform()\n",
    "W1 = tf.Variable(init([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(init([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(init([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(init([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(init([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(init([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "* Tensorflow Optimizers\n",
    "    * tf.keras.optimizers.SGD\n",
    "    * tf.keras.optimizers.Adam\n",
    "    * tf.keras.optimizers.Adagrad\n",
    "    * tf.keras.optimizers.RMSProp\n",
    "\n",
    "\n",
    "![image.png](https://i.imgur.com/JXzrIyO.png)    \n",
    "![image.png](https://cs231n.github.io/assets/nn3/opt1.gif)\n",
    "* Animation : http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "init = tf.initializers.GlorotUniform()\n",
    "W1 = tf.Variable(init([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(init([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(init([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(init([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(init([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(init([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        optimizer.apply_gradients(grads_and_vars=zip([DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6],\\\n",
    "                                                     [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5,  W6, b6]))\n",
    "        '''\n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        '''\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Nueral Net 을 이용한 붗꽃 분류\n",
    "* Dataset 출처 : https://www.openml.org/d/61\n",
    "* CSV 파일 읽기 : pandas.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "csv.iloc[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 데이타 선행 처리\n",
    "* Category to Number :```class, index = np.unique(y, return_inverse=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']])\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "label, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 데이타 선행처리2\n",
    "* 뒤섞기 : ```np.random.shuffle()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "y, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 분류 학습\n",
    "* 입력 : 4\n",
    "* 1층 : 300\n",
    "* 2층 : 100\n",
    "* 출력 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']], dtype=np.float32)\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "y_train = np.eye(n_output)[y_train]\n",
    "y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "y_train, y_test\n",
    "\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]))\n",
    "b1 = tf.Variable(tf.zeros([n_L1]))\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]))\n",
    "b2 = tf.Variable(tf.zeros([n_L2]))\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]))\n",
    "b3 = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "def net(x):\n",
    "    L1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    hypethesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)    \n",
    "    return hypethesis\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = net(X_batch)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([dW1, db1, dW2, db2, dW3, db3], [W1,b1, W2, b2, W3, b3]))\n",
    "            \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}\")\n",
    "                                      \n",
    "is_correct = tf.equal(tf.argmax(net(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out\n",
    "* Overfitting 방지 기술\n",
    "* 학습하는 동안 무작위로 노드를 학습에 제외\n",
    "* Tensorflow\n",
    "    * `tf.nn.dropout(layer, rate)`\n",
    "    * `rate = 0.1 ~1`\n",
    "![image.png](https://i.imgur.com/KtGFdEL.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 분류 Drop out 기법 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']], dtype=np.float32)\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "y_train = np.eye(n_output)[y_train]\n",
    "y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "y_train, y_test\n",
    "\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]))\n",
    "b1 = tf.Variable(tf.zeros([n_L1]))\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]))\n",
    "b2 = tf.Variable(tf.zeros([n_L2]))\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]))\n",
    "b3 = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "def model(x, traning=False):\n",
    "    L1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    if traning:\n",
    "        L2 = tf.nn.dropout(L2, 0.3) # Dropout 추가\n",
    "    hypethesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)    \n",
    "    return hypethesis\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = model(X_batch, traning=True)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([dW1, db1, dW2, db2, dW3, db3], [W1,b1, W2, b2, W3, b3]))\n",
    "            \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}\")\n",
    "                                      \n",
    "is_correct = tf.equal(tf.argmax(model(X_test, traning=False), axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Low-Level API DNN 실습 : MNIST 손글씨 인식\n",
    "#### MNIST Dataset 미리보기\n",
    "* Old Style(Deprecated)\n",
    "    * ```from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)```\n",
    "        * Download URL\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
    "        * save to \"./mnist/data\"\n",
    "        * 70,000개 샘플\n",
    "            * train(55,000), validation(5,000), test(10,000)\n",
    "            * images : m x 784 \n",
    "            * labels : m x 10(one-hot encoding)\n",
    "* Keras\n",
    "    * ```from tensorflow import keras\n",
    "mnist = keras.datasets.mnist.load_data()```\n",
    "        * Download URL : https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "        * Save to : \"~/.keras/datasets/\"\n",
    "        * 70,000개 샘플\n",
    "            * train(60,000), test(10,000)\n",
    "            * images : m x 28 x 28\n",
    "            * label : m x 10(no one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "print(type(mnist), len(mnist))\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "print(X_train.shape, y_test.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(y_train[:5])\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "n_input = 28*28\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 10\n",
    "\n",
    "(X_train, y_train_label), (X_test, y_test_label) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "\n",
    "y_valid_label, y_train_label = y_train_label[:5000], y_train_label[5000:]\n",
    "y_train = np.eye(n_output)[y_train_label]\n",
    "y_valid = np.eye(n_output)[y_valid_label]\n",
    "y_test = np.eye(n_output)[y_test_label]\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]), name=\"W1\")\n",
    "b1 = tf.Variable(tf.zeros([n_L1]), name=\"b1\")\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]), name=\"W2\")\n",
    "b2 = tf.Variable(tf.zeros([n_L2]), name=\"b2\")\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]), name=\"W3\")\n",
    "b3 = tf.Variable(tf.zeros([n_output]), name=\"b3\")\n",
    "\n",
    "\n",
    "def model(X, training=False):\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    if training:\n",
    "        L2 = tf.nn.dropout(L2, 0.3) # Dropout 추가\n",
    "    hyperthesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)\n",
    "    return hyperthesis\n",
    "    \n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "step = 0\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = model(X_batch, training=True)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "        DW1, Db1, DW2, Db2, DW3, Db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([DW1, Db1, DW2, Db2, DW3, Db3], [W1, b1, W2, b2, W3, b3]))\n",
    "    \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))                              \n",
    "    \n",
    "    hypethesis = model(X_valid, training=True)\n",
    "    is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_valid, axis=1))\n",
    "    acc_val = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}, validation accuracy:{acc_val}\")\n",
    "\n",
    "hypethesis = model(X_test)\n",
    "is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))\n",
    "#print(f\"W1:{W1}, b1:{b1}, W2:{W2}, b2:{b2}, W3:{W3}, b3:{b3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 모델 예측\n",
    "* 학습한 모델로 손글씨 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import img2data\n",
    "\n",
    "img_path = './img/0458.png'\n",
    "#img_path = './img/1369.png'\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "numbers = img2data.img2digits(image, (28,28), border=4)\n",
    "for i, n in enumerate(numbers):\n",
    "    print(n.shape)\n",
    "    Z = model(n.astype(np.float32)/255.0)\n",
    "    pred = np.argmax(Z, axis=1)\n",
    "    print(Z, pred)\n",
    "    plt.subplot(1, len(numbers), i+1)\n",
    "    plt.title(str(pred))\n",
    "    plt.imshow(n.reshape(28,28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 붗꽃 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']])\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "#y_train = np.eye(n_output)[y_train]\n",
    "#y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(n_L1, activation=tf.nn.relu, input_shape=(n_input,)),\n",
    "    keras.layers.Dense(n_L2, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(n_output, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30,  batch_size=20)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Accuracy:{}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras MNIST 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import keras\n",
    "import numpy as np\n",
    "import img2data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model_save_path ='./trained_models/keras-mnist.h5'\n",
    "n_input = (28,28)\n",
    "n_L1 = 256\n",
    "n_L2 = 256\n",
    "n_output = 10\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "(X_train, y_train_label), (X_test, y_test_label) = keras.datasets.mnist.load_data()\n",
    "print(X_train.shape)\n",
    "#X_train = X_train.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "#X_test = X_test.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid_label, y_train_label = y_train_label[:5000], y_train_label[5000:]\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=n_input),\n",
    "    keras.layers.Dense(n_L1, activation=tf.nn.relu, input_shape=n_input),\n",
    "    keras.layers.Dense(n_L2, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(n_output, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_label, \n",
    "          epochs=n_epochs,  batch_size=batch_size,\n",
    "          validation_data=(X_valid, y_valid_label), verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_label, verbose=0)\n",
    "\n",
    "print(\"Test Accuracy:{}\".format(test_acc))\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import img2data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_save_path ='./trained_models/keras-mnist.h5'\n",
    "img_path = './img/0458.png'\n",
    "#img_path = './img/1369.png'\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = keras.models.load_model(model_save_path)\n",
    "model.summary()\n",
    "\n",
    "numbers = img2data.img2digits(image, (28,28), reshape=False, border=4)\n",
    "numbers = np.array(numbers)\n",
    "Z = model.predict(numbers)\n",
    "pred = np.argmax(Z, axis=1)\n",
    "print(Z, pred)\n",
    "\n",
    "for i, (n, p) in enumerate(zip(numbers, pred)):\n",
    "    plt.subplot(1, len(numbers), i+1)\n",
    "    plt.title(str(p))\n",
    "    plt.imshow(n.reshape(28,28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 영화리뷰 이진 분류 예제\n",
    "* IMDB Dataset\n",
    "    * IMDB : Internet Movie DataBase\n",
    "    * train 25천개, test : 25천개\n",
    "    * 각각 긍정 부정 50%씩\n",
    "    * 리뷰 데이타는 단어를 숫자 시퀀스로 이미 변환\n",
    "    * 용량 17MB\n",
    "    * keras.datasets.imdb\n",
    "        * load_data(num_words=100000) : 빈도 높은 1만 단어만 사용\n",
    "        * data : 리뷰에 포함된 단어의 인덱스 값들, max:9999\n",
    "            * 각 샘플은 python list 타입, 길이가 제 각각이라서\n",
    "        * label : 0-부정, 1-긍정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 세트와 검증(validation)셋트로 나눠서 fit()함수 호출 이때 `validation_data` 인자 사용\n",
    "    * `history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))`\n",
    "    * 이렇게 하면 `history` 디셔넉리가 'acc', 'loss', 'val_acc', 'val_loss' 4개의 키로 된 결과를 갖는다.\n",
    "        *  훈련 정확도와 손실, 검증 정확도와 손실 값이다.\n",
    "    * plot으로 각 에포크당 훈련결과와 검증 결과를 비교할 수 있다.\n",
    "        * ```\n",
    "        plt.plot(epochs, loss, 'bo', label='training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
    "        ```\n",
    "    * 이걸 보면 어느 에폭에서 과대 적합이 시작되었는지 알 수 있다. 그 에폭과 전체 훈련 셋으로 다시 훈련 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index는 단어와 정수 인덱스를 매핑한 딕셔너리입니다\n",
    "word_index = imdb.get_word_index()\n",
    "# 정수 인덱스와 단어를 매핑하도록 뒤집습니다\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# 리뷰를 디코딩합니다. \n",
    "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺍니다\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 크기가 (len(sequences), dimension))이고 모든 원소가 0인 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # results[i]에서 특정 인덱스의 위치를 1로 만듭니다\n",
    "    return results\n",
    "\n",
    "# 훈련 데이터를 벡터로 변환합니다\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터를 벡터로 변환합니다\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 벡터로 바꿉니다\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
